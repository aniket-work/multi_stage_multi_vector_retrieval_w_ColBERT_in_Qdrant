from sentence_transformers import SentenceTransformer
import numpy as np
from config import EMBEDDING_MODEL


class EmbeddingGenerator:
    def __init__(self, colbert_embedding_dim=12):
        self.model = SentenceTransformer(EMBEDDING_MODEL)
        self.colbert_embedding_dim = colbert_embedding_dim

    def generate_embedding(self, text):
        embedding = self.model.encode(text).tolist()
        print(f"Generated embedding dimension: {len(embedding)}")
        return embedding

    def generate_colbert_embedding(self, text):
        embeddings = self.model.encode(text, output_value='token_embeddings')
        # Flatten the embeddings if they are not already
        if len(embeddings.shape) == 2:
            embeddings = embeddings[0]
        colbert_embedding = embeddings.tolist()

        # Ensure consistent length
        if len(colbert_embedding) < self.colbert_embedding_dim:
            colbert_embedding += [0.0] * (self.colbert_embedding_dim - len(colbert_embedding))
        else:
            colbert_embedding = colbert_embedding[:self.colbert_embedding_dim]

        print(f"Generated ColBERT embedding dimension: {len(colbert_embedding)}")
        return colbert_embedding

    def generate_byte_vector(self, vector, size=4):
        byte_vector = (np.array(vector) * 255).astype(np.uint8)[:size].tolist()
        print(f"Generated byte vector dimension: {len(byte_vector)}")
        return byte_vector


def load_documents(file_path):
    with open(file_path, 'r') as file:
        return [line.strip() for line in file if line.strip()]
